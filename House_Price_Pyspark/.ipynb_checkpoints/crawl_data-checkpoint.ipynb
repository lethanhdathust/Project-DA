{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import cloudscraper\n",
    "import pandas as pd\n",
    "import missingno as msno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl data from Ha Noi\n",
    "\n",
    "offset=0\n",
    "page=1\n",
    "# Get the total post and offset\n",
    "url_hanoi = 'https://gateway.chotot.com/v1/public/ad-listing?region_v2=12000&cg=1000&o={}&page={}&st=s,k&limit=20&w=1&key_param_included=true'\n",
    "url = url_hanoi.format(offset,page)\n",
    "scrape = cloudscraper.create_scraper()\n",
    "content = scrape.get(url_hanoi).text\n",
    "page_content = json.loads(content)\n",
    "len_offset = len(page_content['ads'])\n",
    "total_posts = page_content['total']\n",
    "max_page = total_posts//20 +1\n",
    "list_data_hanoi=[]\n",
    "for i in tqdm (range(1,max_page)):\n",
    "    posts_page = scrape.get(url_hanoi.format(offset,i)).text\n",
    "    offset = (i-1)*20 \n",
    "    if posts_page:\n",
    "        # print(posts_page)\n",
    "        try: \n",
    "            data = json.loads(posts_page)['ads']\n",
    "            if data:\n",
    "                list_data_hanoi.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error:\", e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into DF\n",
    "df_hn=pd.DataFrame()\n",
    "for k in list_data_hanoi:\n",
    "    for i in k:\n",
    "        temp = pd.json_normalize(i)\n",
    "        df_hn= pd.concat([df_hn,temp],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hn.to_csv('ha_noi_data.csv',encoding=\"utf-8-sig\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl data from Ho Chi Minh\n",
    "\n",
    "offset=0\n",
    "page=1\n",
    "# Get the total post and offset\n",
    "url_hcm = 'https://gateway.chotot.com/v1/public/ad-listing?region_v2=13000&cg=1000&o={}&page={}&st=s,k&limit=20&w=1&key_param_included=true'\n",
    "url = url_hcm.format(offset,page)\n",
    "scrape = cloudscraper.create_scraper()\n",
    "content = scrape.get(url_hcm).text\n",
    "page_content = json.loads(content)\n",
    "len_offset = len(page_content['ads'])\n",
    "total_posts = page_content['total']\n",
    "max_page = total_posts//20 +1\n",
    "list_data_hcm=[]\n",
    "for i in tqdm (range(1,max_page)):\n",
    "    posts_page = scrape.get(url_hcm.format(offset,i)).text\n",
    "    offset = (i-1)*20 \n",
    "    if posts_page:\n",
    "        # print(posts_page)\n",
    "        try: \n",
    "            data = json.loads(posts_page)['ads']\n",
    "            if data:\n",
    "                list_data_hcm.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from HCM into DF\n",
    "df_hcm=pd.DataFrame()\n",
    "for k in list_data_hcm:\n",
    "    for i in k:\n",
    "        temp = pd.json_normalize(i)\n",
    "        df_hcm= pd.concat([df_hcm,temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcm= df_hcm.drop_duplicates(keep=False,subset=['ad_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hcm.to_csv('hcm_data_beauti.csv',encoding=\"utf-8-sig\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl data from Da Nang\n",
    "\n",
    "offset=0\n",
    "page=1\n",
    "# Get the total post and offset\n",
    "url_dn = 'https://gateway.chotot.com/v1/public/ad-listing?region_v2=3017&cg=1000&o={}&page={}&st=s,k&limit=20&w=1&key_param_included=true'\n",
    "url = url_dn.format(offset,page)\n",
    "scrape = cloudscraper.create_scraper()\n",
    "content = scrape.get(url_dn).text\n",
    "page_content = json.loads(content)\n",
    "len_offset = len(page_content['ads'])\n",
    "total_posts = page_content['total']\n",
    "max_page = total_posts//20 +1\n",
    "list_data_dn=[]\n",
    "for i in tqdm (range(1,max_page)):\n",
    "    posts_page = scrape.get(url_dn.format(offset,i)).text\n",
    "    offset = (i-1)*20 \n",
    "    if posts_page:\n",
    "        try: \n",
    "            data = json.loads(posts_page)['ads']\n",
    "            if data:\n",
    "                list_data_dn.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "                print(\"JSON Decode Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from DN into DF\n",
    "df_dn=pd.DataFrame()\n",
    "for k in list_data_dn:\n",
    "    for i in k:\n",
    "        temp = pd.json_normalize(i)\n",
    "        df_dn= pd.concat([df_dn,temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dn= df_dn.drop_duplicates(subset=['ad_id'],keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dn.to_csv('dn_data_beauti.csv',encoding=\"utf-8-sig\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the diferent column before merged\n",
    "print(len(df_dn.columns),len(df_hcm.columns),len(df_hn.columns))\n",
    "columns_df_hn=  set(df_hn.columns)\n",
    "columns_df_dn=  set(df_dn.columns)\n",
    "columns_df_hcm=  set(df_hcm.columns)\n",
    "different_columns = columns_df_hn.symmetric_difference(columns_df_dn)\n",
    "print(different_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused column\n",
    "df_hcm=df_hcm.drop(columns='condition_ad', axis=1)\n",
    "df_hn = df_hn.drop(columns='condition_ad',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([df_hn, df_hcm,df_dn], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv('data.csv',encoding=\"utf-8-sig\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
